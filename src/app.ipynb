{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "POS Tagging with HMM and Sentence Generation \n",
    "\n",
    "The training dataset is a subset of the Brown corpus, where each file contains sentences in the form of tokenized words followed by POS tags. Each line contains one sentence. Training dataset can be downloaded from here: https://bit.ly/2kJI0yc The test dataset (which is another subset of the Brown corpus, containing tokenized words but no tags) can be downloaded from here: https://bit.ly/2lMybzP Information regarding the categories of the dataset can be found at: https://bit.ly/2mhF6RT.\n",
    "\n",
    "Your task is to implement a part-of-speech tagger using a bi-gram HMM. Given an observation\n",
    "sequence of n words wn1, choose the most probable sequence of POS tags tn1. For the questionsbelow, please submit both code and output.\n",
    "\n",
    "[Note: During training, for a word to be counted as unknown, the frequency of the word in\n",
    "training set should not exceed a threshold (e.g. 5). You can pick a threshold based on your algorithm design. Also, you can implement smoothing technique based on your own choice, e.g.\n",
    "add-α.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(input_directory):\n",
    "    word_tokens = []\n",
    "    tag_tokens = []\n",
    "    word_tag_tokens = []\n",
    "    transition_tag_tokens = []\n",
    "    sentence_count = 0\n",
    "    for file_name in glob.glob(input_directory + \"/*\"):\n",
    "        print(\"Prepocessing: {}\".format(file_name))\n",
    "        file_pointer = open(file_name, \"r\")\n",
    "        \n",
    "        for line in file_pointer:\n",
    "\n",
    "            # Remove duplicate spaces\n",
    "            file_line_content = re.sub(' +', ' ', line)\n",
    "\n",
    "            # Remove new line characters\n",
    "            file_line_content = line.replace(\"\\n\", \" \")\n",
    "            \n",
    "            # Strip of begin and end spaces\n",
    "            file_line_content = file_line_content.strip()\n",
    "            \n",
    "            # If line is not empty\n",
    "            if file_line_content != \"\":\n",
    "                sentence_count = sentence_count + 1\n",
    "                line_content_list = file_line_content.split(\" \")\n",
    "                \n",
    "                # Append start tag\n",
    "                transition_tag_tokens.append('START')\n",
    "#                 print(line_content_list)\n",
    "                for i in line_content_list:\n",
    "                    word_tag_tokens.append(i)\n",
    "                    split_tokens = i.split('/')\n",
    "                    word = split_tokens[0]\n",
    "                    tag = split_tokens[-1]\n",
    "                    word_tokens.append(word)\n",
    "                    tag_tokens.append(tag)\n",
    "                    transition_tag_tokens.append(tag)\n",
    "    \n",
    "                # Append end tag\n",
    "                transition_tag_tokens.append('END')\n",
    "        \n",
    "        file_pointer.close()\n",
    "        \n",
    "    return sentence_count, word_tokens, tag_tokens, word_tag_tokens, transition_tag_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def replace_low_count_words(word_tokens, cut_off_count, word_tag_tokens):\n",
    "    word_tokens_with_count = Counter(word_tokens)\n",
    "    candidate_words = {}\n",
    "    for word in word_tokens_with_count:\n",
    "        if word_tokens_with_count[word] <= cut_off_count:\n",
    "            candidate_words[word] = 1\n",
    "    \n",
    "    ## Remove all the words <= cut_off_count\n",
    "    for i in range(len(word_tokens)):\n",
    "        if word_tokens[i] in candidate_words:\n",
    "            word_tokens[i] = 'UNK'\n",
    "            split_tokens = word_tag_tokens[i].split('/')\n",
    "            word = split_tokens[0]\n",
    "            tag = split_tokens[-1]\n",
    "            word_tag_tokens[i] = 'UNK' + '/' + tag\n",
    "\n",
    "    return word_tokens, word_tag_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##### Loading train date\n",
      "Prepocessing: ../input/brown_train/cd05\n",
      "Prepocessing: ../input/brown_train/cf37\n",
      "Prepocessing: ../input/brown_train/cf08\n",
      "Prepocessing: ../input/brown_train/cl06\n",
      "Prepocessing: ../input/brown_train/cj68\n",
      "Prepocessing: ../input/brown_train/cf30\n",
      "Prepocessing: ../input/brown_train/cd02\n",
      "Prepocessing: ../input/brown_train/cl01\n",
      "Prepocessing: ../input/brown_train/cj57\n",
      "Prepocessing: ../input/brown_train/cf06\n",
      "Prepocessing: ../input/brown_train/cl08\n",
      "Prepocessing: ../input/brown_train/cj61\n",
      "Prepocessing: ../input/brown_train/cf39\n",
      "Prepocessing: ../input/brown_train/cn05\n",
      "Prepocessing: ../input/brown_train/cj59\n",
      "Prepocessing: ../input/brown_train/cf01\n",
      "Prepocessing: ../input/brown_train/cn02\n",
      "Prepocessing: ../input/brown_train/cj66\n",
      "Prepocessing: ../input/brown_train/cj32\n",
      "Prepocessing: ../input/brown_train/ch07\n",
      "Prepocessing: ../input/brown_train/cj35\n",
      "Prepocessing: ../input/brown_train/cb09\n",
      "Prepocessing: ../input/brown_train/cj03\n",
      "Prepocessing: ../input/brown_train/cj04\n",
      "Prepocessing: ../input/brown_train/ch09\n",
      "Prepocessing: ../input/brown_train/cb07\n",
      "Prepocessing: ../input/brown_train/cj58\n",
      "Prepocessing: ../input/brown_train/cn03\n",
      "Prepocessing: ../input/brown_train/cj67\n",
      "Prepocessing: ../input/brown_train/cl09\n",
      "Prepocessing: ../input/brown_train/cf07\n",
      "Prepocessing: ../input/brown_train/cj60\n",
      "Prepocessing: ../input/brown_train/cn04\n",
      "Prepocessing: ../input/brown_train/cf38\n",
      "Prepocessing: ../input/brown_train/cj69\n",
      "Prepocessing: ../input/brown_train/cd03\n",
      "Prepocessing: ../input/brown_train/cf31\n",
      "Prepocessing: ../input/brown_train/cj56\n",
      "Prepocessing: ../input/brown_train/cf36\n",
      "Prepocessing: ../input/brown_train/cd04\n",
      "Prepocessing: ../input/brown_train/cj51\n",
      "Prepocessing: ../input/brown_train/cl07\n",
      "Prepocessing: ../input/brown_train/cf09\n",
      "Prepocessing: ../input/brown_train/cj05\n",
      "Prepocessing: ../input/brown_train/cb06\n",
      "Prepocessing: ../input/brown_train/cj02\n",
      "Prepocessing: ../input/brown_train/ch30\n",
      "Prepocessing: ../input/brown_train/cb01\n",
      "Prepocessing: ../input/brown_train/cb08\n",
      "Prepocessing: ../input/brown_train/cj34\n",
      "Prepocessing: ../input/brown_train/ch06\n",
      "Prepocessing: ../input/brown_train/ch01\n",
      "Prepocessing: ../input/brown_train/cj33\n",
      "Prepocessing: ../input/brown_train/cr03\n",
      "Prepocessing: ../input/brown_train/ce14\n",
      "Prepocessing: ../input/brown_train/cg26\n",
      "Prepocessing: ../input/brown_train/cg19\n",
      "Prepocessing: ../input/brown_train/cg21\n",
      "Prepocessing: ../input/brown_train/ce13\n",
      "Prepocessing: ../input/brown_train/cg17\n",
      "Prepocessing: ../input/brown_train/ce25\n",
      "Prepocessing: ../input/brown_train/ca41\n",
      "Prepocessing: ../input/brown_train/cg28\n",
      "Prepocessing: ../input/brown_train/cp07\n",
      "Prepocessing: ../input/brown_train/ce22\n",
      "Prepocessing: ../input/brown_train/cg10\n",
      "Prepocessing: ../input/brown_train/ck23\n",
      "Prepocessing: ../input/brown_train/ca12\n",
      "Prepocessing: ../input/brown_train/cg44\n",
      "Prepocessing: ../input/brown_train/ck24\n",
      "Prepocessing: ../input/brown_train/cg43\n",
      "Prepocessing: ../input/brown_train/ca15\n",
      "Prepocessing: ../input/brown_train/ck12\n",
      "Prepocessing: ../input/brown_train/cc11\n",
      "Prepocessing: ../input/brown_train/ca23\n",
      "Prepocessing: ../input/brown_train/cg75\n",
      "Prepocessing: ../input/brown_train/cg72\n",
      "Prepocessing: ../input/brown_train/ca24\n",
      "Prepocessing: ../input/brown_train/cc16\n",
      "Prepocessing: ../input/brown_train/cg11\n",
      "Prepocessing: ../input/brown_train/ce23\n",
      "Prepocessing: ../input/brown_train/cp06\n",
      "Prepocessing: ../input/brown_train/cp01\n",
      "Prepocessing: ../input/brown_train/ce24\n",
      "Prepocessing: ../input/brown_train/cg16\n",
      "Prepocessing: ../input/brown_train/ca40\n",
      "Prepocessing: ../input/brown_train/cg29\n",
      "Prepocessing: ../input/brown_train/cr05\n",
      "Prepocessing: ../input/brown_train/ce12\n",
      "Prepocessing: ../input/brown_train/cg20\n",
      "Prepocessing: ../input/brown_train/cp08\n",
      "Prepocessing: ../input/brown_train/cg27\n",
      "Prepocessing: ../input/brown_train/ce15\n",
      "Prepocessing: ../input/brown_train/cr02\n",
      "Prepocessing: ../input/brown_train/cg18\n",
      "Prepocessing: ../input/brown_train/ck14\n",
      "Prepocessing: ../input/brown_train/cg73\n",
      "Prepocessing: ../input/brown_train/cc17\n",
      "Prepocessing: ../input/brown_train/ca25\n",
      "Prepocessing: ../input/brown_train/ck13\n",
      "Prepocessing: ../input/brown_train/ca22\n",
      "Prepocessing: ../input/brown_train/cc10\n",
      "Prepocessing: ../input/brown_train/cg74\n",
      "Prepocessing: ../input/brown_train/ck25\n",
      "Prepocessing: ../input/brown_train/cg42\n",
      "Prepocessing: ../input/brown_train/ca14\n",
      "Prepocessing: ../input/brown_train/ck22\n",
      "Prepocessing: ../input/brown_train/ca13\n",
      "Prepocessing: ../input/brown_train/cg45\n",
      "Prepocessing: ../input/brown_train/cg67\n",
      "Prepocessing: ../input/brown_train/ca31\n",
      "Prepocessing: ../input/brown_train/cc03\n",
      "Prepocessing: ../input/brown_train/cg58\n",
      "Prepocessing: ../input/brown_train/cc04\n",
      "Prepocessing: ../input/brown_train/ca36\n",
      "Prepocessing: ../input/brown_train/cg60\n",
      "Prepocessing: ../input/brown_train/ca09\n",
      "Prepocessing: ../input/brown_train/ck07\n",
      "Prepocessing: ../input/brown_train/cg56\n",
      "Prepocessing: ../input/brown_train/cg69\n",
      "Prepocessing: ../input/brown_train/ca07\n",
      "Prepocessing: ../input/brown_train/ck09\n",
      "Prepocessing: ../input/brown_train/cg51\n",
      "Prepocessing: ../input/brown_train/ca38\n",
      "Prepocessing: ../input/brown_train/ce08\n",
      "Prepocessing: ../input/brown_train/cg05\n",
      "Prepocessing: ../input/brown_train/cp12\n",
      "Prepocessing: ../input/brown_train/cp15\n",
      "Prepocessing: ../input/brown_train/cg02\n",
      "Prepocessing: ../input/brown_train/ce30\n",
      "Prepocessing: ../input/brown_train/cm05\n",
      "Prepocessing: ../input/brown_train/cp23\n",
      "Prepocessing: ../input/brown_train/cg34\n",
      "Prepocessing: ../input/brown_train/ce06\n",
      "Prepocessing: ../input/brown_train/cm02\n",
      "Prepocessing: ../input/brown_train/ce01\n",
      "Prepocessing: ../input/brown_train/cg33\n",
      "Prepocessing: ../input/brown_train/cp24\n",
      "Prepocessing: ../input/brown_train/ck08\n",
      "Prepocessing: ../input/brown_train/ca06\n",
      "Prepocessing: ../input/brown_train/cg50\n",
      "Prepocessing: ../input/brown_train/ca39\n",
      "Prepocessing: ../input/brown_train/ca01\n",
      "Prepocessing: ../input/brown_train/cg68\n",
      "Prepocessing: ../input/brown_train/ca37\n",
      "Prepocessing: ../input/brown_train/cc05\n",
      "Prepocessing: ../input/brown_train/cg61\n",
      "Prepocessing: ../input/brown_train/ck06\n",
      "Prepocessing: ../input/brown_train/ca08\n",
      "Prepocessing: ../input/brown_train/cg66\n",
      "Prepocessing: ../input/brown_train/cc02\n",
      "Prepocessing: ../input/brown_train/ca30\n",
      "Prepocessing: ../input/brown_train/ck01\n",
      "Prepocessing: ../input/brown_train/cg59\n",
      "Prepocessing: ../input/brown_train/cp25\n",
      "Prepocessing: ../input/brown_train/cg32\n",
      "Prepocessing: ../input/brown_train/cm04\n",
      "Prepocessing: ../input/brown_train/ce07\n",
      "Prepocessing: ../input/brown_train/cg35\n",
      "Prepocessing: ../input/brown_train/cp22\n",
      "Prepocessing: ../input/brown_train/ce31\n",
      "Prepocessing: ../input/brown_train/cg03\n",
      "Prepocessing: ../input/brown_train/cp14\n",
      "Prepocessing: ../input/brown_train/ce09\n",
      "Prepocessing: ../input/brown_train/cp13\n",
      "Prepocessing: ../input/brown_train/cg04\n",
      "Prepocessing: ../input/brown_train/ce36\n",
      "Prepocessing: ../input/brown_train/cb12\n",
      "Prepocessing: ../input/brown_train/cj11\n",
      "Prepocessing: ../input/brown_train/ch23\n",
      "Prepocessing: ../input/brown_train/cb15\n",
      "Prepocessing: ../input/brown_train/cj29\n",
      "Prepocessing: ../input/brown_train/ch24\n",
      "Prepocessing: ../input/brown_train/cj16\n",
      "Prepocessing: ../input/brown_train/cf47\n",
      "Prepocessing: ../input/brown_train/cb23\n",
      "Prepocessing: ../input/brown_train/ch12\n",
      "Prepocessing: ../input/brown_train/cj20\n",
      "Prepocessing: ../input/brown_train/cb24\n",
      "Prepocessing: ../input/brown_train/cj18\n",
      "Prepocessing: ../input/brown_train/cf40\n",
      "Prepocessing: ../input/brown_train/cj27\n",
      "Prepocessing: ../input/brown_train/ch15\n",
      "Prepocessing: ../input/brown_train/cj73\n",
      "Prepocessing: ../input/brown_train/cn28\n",
      "Prepocessing: ../input/brown_train/cj80\n",
      "Prepocessing: ../input/brown_train/cj74\n",
      "Prepocessing: ../input/brown_train/cl22\n",
      "Prepocessing: ../input/brown_train/cn10\n",
      "Prepocessing: ../input/brown_train/cf13\n",
      "Prepocessing: ../input/brown_train/cl14\n",
      "Prepocessing: ../input/brown_train/cn26\n",
      "Prepocessing: ../input/brown_train/cj42\n",
      "Prepocessing: ../input/brown_train/cn19\n",
      "Prepocessing: ../input/brown_train/cf25\n",
      "Prepocessing: ../input/brown_train/cd17\n",
      "Prepocessing: ../input/brown_train/cj45\n",
      "Prepocessing: ../input/brown_train/cn21\n",
      "Prepocessing: ../input/brown_train/cl13\n",
      "Prepocessing: ../input/brown_train/cd10\n",
      "Prepocessing: ../input/brown_train/cf22\n",
      "Prepocessing: ../input/brown_train/cj19\n",
      "Prepocessing: ../input/brown_train/cf41\n",
      "Prepocessing: ../input/brown_train/ch14\n",
      "Prepocessing: ../input/brown_train/cj26\n",
      "Prepocessing: ../input/brown_train/cf46\n",
      "Prepocessing: ../input/brown_train/cb22\n",
      "Prepocessing: ../input/brown_train/cj21\n",
      "Prepocessing: ../input/brown_train/ch13\n",
      "Prepocessing: ../input/brown_train/cj28\n",
      "Prepocessing: ../input/brown_train/cb14\n",
      "Prepocessing: ../input/brown_train/cj17\n",
      "Prepocessing: ../input/brown_train/ch25\n",
      "Prepocessing: ../input/brown_train/cb13\n",
      "Prepocessing: ../input/brown_train/ch22\n",
      "Prepocessing: ../input/brown_train/cj10\n",
      "Prepocessing: ../input/brown_train/cf48\n",
      "Prepocessing: ../input/brown_train/cj44\n",
      "Prepocessing: ../input/brown_train/cl12\n",
      "Prepocessing: ../input/brown_train/cn20\n",
      "Prepocessing: ../input/brown_train/cf23\n",
      "Prepocessing: ../input/brown_train/cd11\n",
      "Prepocessing: ../input/brown_train/cn27\n",
      "Prepocessing: ../input/brown_train/cl15\n",
      "Prepocessing: ../input/brown_train/cj43\n",
      "Prepocessing: ../input/brown_train/cd16\n",
      "Prepocessing: ../input/brown_train/cf24\n",
      "Prepocessing: ../input/brown_train/cn18\n",
      "Prepocessing: ../input/brown_train/cj75\n",
      "Prepocessing: ../input/brown_train/cn11\n",
      "Prepocessing: ../input/brown_train/cl23\n",
      "Prepocessing: ../input/brown_train/cf12\n",
      "Prepocessing: ../input/brown_train/cl24\n",
      "Prepocessing: ../input/brown_train/cn16\n",
      "Prepocessing: ../input/brown_train/cj72\n",
      "Prepocessing: ../input/brown_train/cf15\n",
      "Prepocessing: ../input/brown_train/cn29\n",
      "Prepocessing: ../input/brown_train/cj36\n",
      "Prepocessing: ../input/brown_train/ch04\n",
      "Prepocessing: ../input/brown_train/cj09\n",
      "Prepocessing: ../input/brown_train/ch03\n",
      "Prepocessing: ../input/brown_train/cj31\n",
      "Prepocessing: ../input/brown_train/cj07\n",
      "Prepocessing: ../input/brown_train/cj38\n",
      "Prepocessing: ../input/brown_train/cb04\n",
      "Prepocessing: ../input/brown_train/cb03\n",
      "Prepocessing: ../input/brown_train/cd01\n",
      "Prepocessing: ../input/brown_train/cf33\n",
      "Prepocessing: ../input/brown_train/cl02\n",
      "Prepocessing: ../input/brown_train/cj54\n",
      "Prepocessing: ../input/brown_train/cf34\n",
      "Prepocessing: ../input/brown_train/cn08\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepocessing: ../input/brown_train/cd06\n",
      "Prepocessing: ../input/brown_train/cj53\n",
      "Prepocessing: ../input/brown_train/cl05\n",
      "Prepocessing: ../input/brown_train/cf02\n",
      "Prepocessing: ../input/brown_train/cn01\n",
      "Prepocessing: ../input/brown_train/cj65\n",
      "Prepocessing: ../input/brown_train/cf05\n",
      "Prepocessing: ../input/brown_train/cj62\n",
      "Prepocessing: ../input/brown_train/cn06\n",
      "Prepocessing: ../input/brown_train/cd08\n",
      "Prepocessing: ../input/brown_train/cj01\n",
      "Prepocessing: ../input/brown_train/cb02\n",
      "Prepocessing: ../input/brown_train/cj06\n",
      "Prepocessing: ../input/brown_train/cb05\n",
      "Prepocessing: ../input/brown_train/cj39\n",
      "Prepocessing: ../input/brown_train/cj30\n",
      "Prepocessing: ../input/brown_train/ch02\n",
      "Prepocessing: ../input/brown_train/ch05\n",
      "Prepocessing: ../input/brown_train/cj37\n",
      "Prepocessing: ../input/brown_train/cj08\n",
      "Prepocessing: ../input/brown_train/cf04\n",
      "Prepocessing: ../input/brown_train/cj63\n",
      "Prepocessing: ../input/brown_train/cd09\n",
      "Prepocessing: ../input/brown_train/cn07\n",
      "Prepocessing: ../input/brown_train/cf03\n",
      "Prepocessing: ../input/brown_train/cj64\n",
      "Prepocessing: ../input/brown_train/cd07\n",
      "Prepocessing: ../input/brown_train/cn09\n",
      "Prepocessing: ../input/brown_train/cf35\n",
      "Prepocessing: ../input/brown_train/cj52\n",
      "Prepocessing: ../input/brown_train/cl04\n",
      "Prepocessing: ../input/brown_train/cf32\n",
      "Prepocessing: ../input/brown_train/cl03\n",
      "Prepocessing: ../input/brown_train/cj55\n",
      "Prepocessing: ../input/brown_train/ck27\n",
      "Prepocessing: ../input/brown_train/ca29\n",
      "Prepocessing: ../input/brown_train/cg40\n",
      "Prepocessing: ../input/brown_train/ca16\n",
      "Prepocessing: ../input/brown_train/ck18\n",
      "Prepocessing: ../input/brown_train/ck20\n",
      "Prepocessing: ../input/brown_train/ca11\n",
      "Prepocessing: ../input/brown_train/cg47\n",
      "Prepocessing: ../input/brown_train/ca18\n",
      "Prepocessing: ../input/brown_train/ck16\n",
      "Prepocessing: ../input/brown_train/cg71\n",
      "Prepocessing: ../input/brown_train/cc15\n",
      "Prepocessing: ../input/brown_train/ck29\n",
      "Prepocessing: ../input/brown_train/ca27\n",
      "Prepocessing: ../input/brown_train/cg49\n",
      "Prepocessing: ../input/brown_train/ck11\n",
      "Prepocessing: ../input/brown_train/ca20\n",
      "Prepocessing: ../input/brown_train/cc12\n",
      "Prepocessing: ../input/brown_train/ce10\n",
      "Prepocessing: ../input/brown_train/cg22\n",
      "Prepocessing: ../input/brown_train/cr07\n",
      "Prepocessing: ../input/brown_train/cg25\n",
      "Prepocessing: ../input/brown_train/ce17\n",
      "Prepocessing: ../input/brown_train/ce28\n",
      "Prepocessing: ../input/brown_train/cp04\n",
      "Prepocessing: ../input/brown_train/cg13\n",
      "Prepocessing: ../input/brown_train/ce21\n",
      "Prepocessing: ../input/brown_train/cr09\n",
      "Prepocessing: ../input/brown_train/ce26\n",
      "Prepocessing: ../input/brown_train/cg14\n",
      "Prepocessing: ../input/brown_train/ca42\n",
      "Prepocessing: ../input/brown_train/cp03\n",
      "Prepocessing: ../input/brown_train/cg48\n",
      "Prepocessing: ../input/brown_train/ck10\n",
      "Prepocessing: ../input/brown_train/cc13\n",
      "Prepocessing: ../input/brown_train/ca21\n",
      "Prepocessing: ../input/brown_train/ck17\n",
      "Prepocessing: ../input/brown_train/ca19\n",
      "Prepocessing: ../input/brown_train/cg70\n",
      "Prepocessing: ../input/brown_train/ca26\n",
      "Prepocessing: ../input/brown_train/ck28\n",
      "Prepocessing: ../input/brown_train/cc14\n",
      "Prepocessing: ../input/brown_train/ck21\n",
      "Prepocessing: ../input/brown_train/ca10\n",
      "Prepocessing: ../input/brown_train/cg46\n",
      "Prepocessing: ../input/brown_train/ca28\n",
      "Prepocessing: ../input/brown_train/ck26\n",
      "Prepocessing: ../input/brown_train/cg41\n",
      "Prepocessing: ../input/brown_train/ck19\n",
      "Prepocessing: ../input/brown_train/ca17\n",
      "Prepocessing: ../input/brown_train/cp02\n",
      "Prepocessing: ../input/brown_train/cg15\n",
      "Prepocessing: ../input/brown_train/ce27\n",
      "Prepocessing: ../input/brown_train/ca43\n",
      "Prepocessing: ../input/brown_train/ce18\n",
      "Prepocessing: ../input/brown_train/ca44\n",
      "Prepocessing: ../input/brown_train/ce20\n",
      "Prepocessing: ../input/brown_train/cg12\n",
      "Prepocessing: ../input/brown_train/cp05\n",
      "Prepocessing: ../input/brown_train/cr08\n",
      "Prepocessing: ../input/brown_train/ce16\n",
      "Prepocessing: ../input/brown_train/cg24\n",
      "Prepocessing: ../input/brown_train/cr01\n",
      "Prepocessing: ../input/brown_train/ce29\n",
      "Prepocessing: ../input/brown_train/cr06\n",
      "Prepocessing: ../input/brown_train/cg23\n",
      "Prepocessing: ../input/brown_train/ce11\n",
      "Prepocessing: ../input/brown_train/cp29\n",
      "Prepocessing: ../input/brown_train/cp16\n",
      "Prepocessing: ../input/brown_train/ce33\n",
      "Prepocessing: ../input/brown_train/cg01\n",
      "Prepocessing: ../input/brown_train/cg39\n",
      "Prepocessing: ../input/brown_train/cg06\n",
      "Prepocessing: ../input/brown_train/ce34\n",
      "Prepocessing: ../input/brown_train/cp11\n",
      "Prepocessing: ../input/brown_train/cm01\n",
      "Prepocessing: ../input/brown_train/cp18\n",
      "Prepocessing: ../input/brown_train/cg30\n",
      "Prepocessing: ../input/brown_train/ce02\n",
      "Prepocessing: ../input/brown_train/cp27\n",
      "Prepocessing: ../input/brown_train/cg08\n",
      "Prepocessing: ../input/brown_train/cm06\n",
      "Prepocessing: ../input/brown_train/cp20\n",
      "Prepocessing: ../input/brown_train/ce05\n",
      "Prepocessing: ../input/brown_train/cg37\n",
      "Prepocessing: ../input/brown_train/ca35\n",
      "Prepocessing: ../input/brown_train/cg63\n",
      "Prepocessing: ../input/brown_train/ck04\n",
      "Prepocessing: ../input/brown_train/cg64\n",
      "Prepocessing: ../input/brown_train/ca32\n",
      "Prepocessing: ../input/brown_train/ck03\n",
      "Prepocessing: ../input/brown_train/ca04\n",
      "Prepocessing: ../input/brown_train/cg52\n",
      "Prepocessing: ../input/brown_train/cc09\n",
      "Prepocessing: ../input/brown_train/cg55\n",
      "Prepocessing: ../input/brown_train/ca03\n",
      "Prepocessing: ../input/brown_train/cg09\n",
      "Prepocessing: ../input/brown_train/cg36\n",
      "Prepocessing: ../input/brown_train/ce04\n",
      "Prepocessing: ../input/brown_train/cp21\n",
      "Prepocessing: ../input/brown_train/cp19\n",
      "Prepocessing: ../input/brown_train/cp26\n",
      "Prepocessing: ../input/brown_train/ce03\n",
      "Prepocessing: ../input/brown_train/cg31\n",
      "Prepocessing: ../input/brown_train/cg38\n",
      "Prepocessing: ../input/brown_train/cp10\n",
      "Prepocessing: ../input/brown_train/ce35\n",
      "Prepocessing: ../input/brown_train/cg07\n",
      "Prepocessing: ../input/brown_train/cp28\n",
      "Prepocessing: ../input/brown_train/ce32\n",
      "Prepocessing: ../input/brown_train/cp17\n",
      "Prepocessing: ../input/brown_train/cg54\n",
      "Prepocessing: ../input/brown_train/ca02\n",
      "Prepocessing: ../input/brown_train/ca05\n",
      "Prepocessing: ../input/brown_train/cg53\n",
      "Prepocessing: ../input/brown_train/cc08\n",
      "Prepocessing: ../input/brown_train/cg65\n",
      "Prepocessing: ../input/brown_train/ca33\n",
      "Prepocessing: ../input/brown_train/cc01\n",
      "Prepocessing: ../input/brown_train/ck02\n",
      "Prepocessing: ../input/brown_train/cc06\n",
      "Prepocessing: ../input/brown_train/cg62\n",
      "Prepocessing: ../input/brown_train/ck05\n",
      "Prepocessing: ../input/brown_train/cj77\n",
      "Prepocessing: ../input/brown_train/cn13\n",
      "Prepocessing: ../input/brown_train/cf10\n",
      "Prepocessing: ../input/brown_train/cj48\n",
      "Prepocessing: ../input/brown_train/cn14\n",
      "Prepocessing: ../input/brown_train/cf28\n",
      "Prepocessing: ../input/brown_train/cj70\n",
      "Prepocessing: ../input/brown_train/cf17\n",
      "Prepocessing: ../input/brown_train/cl19\n",
      "Prepocessing: ../input/brown_train/cj46\n",
      "Prepocessing: ../input/brown_train/cl10\n",
      "Prepocessing: ../input/brown_train/cn22\n",
      "Prepocessing: ../input/brown_train/cf21\n",
      "Prepocessing: ../input/brown_train/cd13\n",
      "Prepocessing: ../input/brown_train/cj79\n",
      "Prepocessing: ../input/brown_train/cn25\n",
      "Prepocessing: ../input/brown_train/cf19\n",
      "Prepocessing: ../input/brown_train/cl17\n",
      "Prepocessing: ../input/brown_train/cj41\n",
      "Prepocessing: ../input/brown_train/cf26\n",
      "Prepocessing: ../input/brown_train/ch18\n",
      "Prepocessing: ../input/brown_train/cb16\n",
      "Prepocessing: ../input/brown_train/cj15\n",
      "Prepocessing: ../input/brown_train/ch27\n",
      "Prepocessing: ../input/brown_train/cb11\n",
      "Prepocessing: ../input/brown_train/ch20\n",
      "Prepocessing: ../input/brown_train/cj12\n",
      "Prepocessing: ../input/brown_train/cb27\n",
      "Prepocessing: ../input/brown_train/ch29\n",
      "Prepocessing: ../input/brown_train/cf43\n",
      "Prepocessing: ../input/brown_train/ch16\n",
      "Prepocessing: ../input/brown_train/cb18\n",
      "Prepocessing: ../input/brown_train/cj24\n",
      "Prepocessing: ../input/brown_train/cf44\n",
      "Prepocessing: ../input/brown_train/cb20\n",
      "Prepocessing: ../input/brown_train/cj23\n",
      "Prepocessing: ../input/brown_train/ch11\n",
      "Prepocessing: ../input/brown_train/cl16\n",
      "Prepocessing: ../input/brown_train/cf18\n",
      "Prepocessing: ../input/brown_train/cn24\n",
      "Prepocessing: ../input/brown_train/cj40\n",
      "Prepocessing: ../input/brown_train/cf27\n",
      "Prepocessing: ../input/brown_train/cd15\n",
      "Prepocessing: ../input/brown_train/cj47\n",
      "Prepocessing: ../input/brown_train/cn23\n",
      "Prepocessing: ../input/brown_train/cl11\n",
      "Prepocessing: ../input/brown_train/cd12\n",
      "Prepocessing: ../input/brown_train/cf20\n",
      "Prepocessing: ../input/brown_train/cj78\n",
      "Prepocessing: ../input/brown_train/cf29\n",
      "Prepocessing: ../input/brown_train/cn15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepocessing: ../input/brown_train/cj71\n",
      "Prepocessing: ../input/brown_train/cl18\n",
      "Prepocessing: ../input/brown_train/cf16\n",
      "Prepocessing: ../input/brown_train/cj76\n",
      "Prepocessing: ../input/brown_train/cl20\n",
      "Prepocessing: ../input/brown_train/cn12\n",
      "Prepocessing: ../input/brown_train/cf11\n",
      "Prepocessing: ../input/brown_train/cj49\n",
      "Prepocessing: ../input/brown_train/cf45\n",
      "Prepocessing: ../input/brown_train/cb21\n",
      "Prepocessing: ../input/brown_train/ch10\n",
      "Prepocessing: ../input/brown_train/cj22\n",
      "Prepocessing: ../input/brown_train/ch28\n",
      "Prepocessing: ../input/brown_train/cb26\n",
      "Prepocessing: ../input/brown_train/cf42\n",
      "Prepocessing: ../input/brown_train/cj25\n",
      "Prepocessing: ../input/brown_train/cb19\n",
      "Prepocessing: ../input/brown_train/ch17\n",
      "Prepocessing: ../input/brown_train/cb10\n",
      "Prepocessing: ../input/brown_train/cj13\n",
      "Prepocessing: ../input/brown_train/ch21\n",
      "Prepocessing: ../input/brown_train/cb17\n",
      "Prepocessing: ../input/brown_train/ch19\n",
      "Prepocessing: ../input/brown_train/ch26\n",
      "Prepocessing: ../input/brown_train/cj14\n",
      " Number of Sentences: 55684, Word list count: 1126281, Tag list count: 1126281, Transition Tag list count: 1126281 Word Tag List\n",
      "### Replace word tokens <= 5 with 'UNK\n",
      "Number of tokens after replacement: Word - 1126281, Word_Tag - 1126281\n"
     ]
    }
   ],
   "source": [
    "print(\"##### Loading train date\")\n",
    "# train_sentence_count, train_word_tokens, train_tag_tokens, train_word_tag_tokens, \\\n",
    "#     train_transition_tag_tokens = load_data('../input/custom_train')\n",
    "    \n",
    "train_sentence_count, train_word_tokens, train_tag_tokens, train_word_tag_tokens, \\\n",
    "    train_transition_tag_tokens = load_data('../input/brown_train')\n",
    "    \n",
    "print(\" Number of Sentences: {}, Word list count: {}, Tag list count: {}, Transition Tag list count: {}\"\\\n",
    "      \" Word Tag List\"\n",
    "       .format(train_sentence_count, len(train_word_tokens), len(train_tag_tokens), len(train_word_tag_tokens), len(train_transition_tag_tokens))\n",
    "      )\n",
    "\n",
    "print(\"### Replace word tokens <= 5 with 'UNK\")\n",
    "train_word_tokens, train_word_tag_tokens = replace_low_count_words(train_word_tokens, 1, train_word_tag_tokens)\n",
    "print(\"Number of tokens after replacement: Word - {}, Word_Tag - {}\".format(len(train_word_tokens), len(train_word_tag_tokens)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.1\n",
    "Obtain frequency counts from the collection of all the training files (counted together). You will need the following types of frequency counts: word-tag counts, tag un-igram counts, and tag bigram counts. Let’s denote these by C(wi, ti), C(ti) and C(ti−1, ti) respectively. Report these quantities in different output files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_unigrams(token_list):\n",
    "    tokens_with_count = Counter(token_list)\n",
    "    return tokens_with_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_ngrams(token_list, n):\n",
    "    ngrams_zip = zip(*[token_list[i:] for i in range(n)])\n",
    "    ngrams_list = [\" \".join(element) for element in ngrams_zip]\n",
    "    ngrams_keys_counts = Counter(ngrams_list)\n",
    "    return ngrams_keys_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigragms for words\n",
      "30036\n",
      "Vocab size 30036\n",
      "Unigragms for word-tag\n",
      "1126281\n",
      "41029\n",
      "Unigragms for tags\n",
      "470\n",
      "Unigrams for transition tags\n",
      "472\n",
      "Bigrams for transition tags\n",
      "8255\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigragms for words\")\n",
    "train_word_unigrams = get_unigrams(train_word_tokens)\n",
    "print(len(train_word_unigrams))\n",
    "vocab_size = len(train_word_unigrams)\n",
    "print(\"Vocab size {}\".format(vocab_size))\n",
    "\n",
    "print(\"Unigragms for word-tag\")\n",
    "print(len(train_word_tag_tokens))\n",
    "train_word_tag_unigrams = get_unigrams(train_word_tag_tokens)\n",
    "with open('../output/word_tag_unigrams.txt', 'w') as word_tag_unigrams_output_file:\n",
    "    word_tag_unigrams_output_file.write(str(train_word_tag_unigrams))\n",
    "print(len(train_word_tag_unigrams))\n",
    "\n",
    "print(\"Unigragms for tags\")\n",
    "train_tag_unigrams = get_unigrams(train_tag_tokens)\n",
    "print(len(train_tag_unigrams))\n",
    "\n",
    "print(\"Unigrams for transition tags\")\n",
    "train_transition_tag_unigrams = get_unigrams(train_transition_tag_tokens)\n",
    "with open('../output/tag_unigrams.txt', 'w') as transition_tag_unigrams_output_file:\n",
    "    transition_tag_unigrams_output_file.write(str(train_transition_tag_unigrams))\n",
    "print(len(train_transition_tag_unigrams))\n",
    "\n",
    "print(\"Bigrams for transition tags\")\n",
    "train_transition_tag_bigrams = get_ngrams(train_transition_tag_tokens, 2)\n",
    "with open('../output/tag_bigrams.txt', 'w') as transition_tag_bigrams_output_file:\n",
    "    transition_tag_bigrams_output_file.write(str(train_transition_tag_bigrams))\n",
    "print(len(train_transition_tag_bigrams))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.2  \n",
    "A transition probability is the probability of a tag given its previous tag. Calculate transition\n",
    "probabilities of the training set using the following equation:  \n",
    "P(ti−1, ti) = C(ti−1, ti)/C(ti−1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_transition_probability(tag_unigrams, tag_bigrams, lambda_value, vocab_size):\n",
    "    transition_probability = {}\n",
    "    for i in tag_bigrams:\n",
    "        previous_tag = i.split(\" \")[0]\n",
    "        transition_probability[i] = (tag_bigrams[i] + lambda_value) / (tag_unigrams[previous_tag] + (lambda_value * vocab_size))\n",
    "    return transition_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get transition probability\n"
     ]
    }
   ],
   "source": [
    "print(\"Get transition probability\")\n",
    "train_transition_probability = get_transition_probability(train_transition_tag_unigrams, train_transition_tag_bigrams, 0.1, vocab_size)\n",
    "# print(train_transition_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.3  \n",
    "An emission probability is the probability of a given word being associated with a given tag. Calculate emission probabilities of the training set using the following equation:  \n",
    "P(wi, ti) = C(wi, ti)/C(ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_emission_probability(word_tag_unigrams, tag_unigrams, lambda_value, vocab_size):\n",
    "    emission_probability = {}\n",
    "    for i in word_tag_unigrams:\n",
    "        split_tokens = i.split('/')\n",
    "        word = split_tokens[0]\n",
    "        tag = split_tokens[-1]\n",
    "        emission_probability[i] = (word_tag_unigrams[i] + lambda_value) / (tag_unigrams[tag] + (lambda_value * vocab_size))\n",
    "#         print(word, tag)\n",
    "#         print(word_tag_unigrams[i], lambda_value)\n",
    "#         print(tag_unigrams[tag], lambda_value, vocab_size)\n",
    "#         print(emission_probability[i])\n",
    "    return emission_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get emission probability\n"
     ]
    }
   ],
   "source": [
    "print(\"Get emission probability\")\n",
    "train_emission_probability = get_emission_probability(train_word_tag_unigrams, train_transition_tag_unigrams, 0.1, vocab_size)\n",
    "# print(train_emission_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.4 \n",
    "Generate 5 random sentences using the previously learned HMM. Output each sentence (with the POS tags) and its probability of being generated.\n",
    "\n",
    "Hint:\n",
    "With the help of emission probabilities and transition probabilities collected from 4.2 and 4.3,\n",
    "    1. Start with ‘<start>’ tag.\n",
    "    2. Choose next tag based on random choice but considering probabilities e.g. tag draw = random.choices(<tags>,<tag-probabilities>,<numberof tags to draw>).\n",
    "    3. Now choose word for the corresponding tag using emission probabilities (all the words that can be generated from that tag and corresponding probabilities they can be generated with.)e.g. word draw = random.choices(<words>,<word-probabilities>,<numberof tags to draw>).\n",
    "    4. Keep repeating steps 2 and 3 till you hit end token ‘</end>.’\n",
    "    5. Report the sentence and the probability with which this sentence can be generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(transition_probability, emission_probability):\n",
    "    \n",
    "    sentence_tags = []\n",
    "    sentence_words = []\n",
    "    sentence_transition_probability = []\n",
    "    sentence_emission_probability = []\n",
    "    \n",
    "    while(True):\n",
    "        # Get potetial next set of tags and its probabilities\n",
    "        current_tag = \"START\"\n",
    "        next_tag = []\n",
    "        next_tag_probabilities = []\n",
    "        next_tag_dict = {}\n",
    "        for i in transition_probability:\n",
    "            if i.split(\" \")[0] == current_tag and i != \"END START\":\n",
    "                tag = i.split(\" \")[1]\n",
    "                probability = transition_probability[i]\n",
    "                next_tag.append(tag)\n",
    "                next_tag_probabilities.append(probability)\n",
    "                next_tag_dict[tag] = probability\n",
    "                \n",
    "        # Random pick of a tag\n",
    "        tag_drawn = random.choices(next_tag, next_tag_probabilities)[0]\n",
    "        \n",
    "#         print(\"Tag\")\n",
    "#         print(next_tag)\n",
    "#         print(next_tag_probabilities)\n",
    "#         print(tag_drawn)\n",
    "        \n",
    "        if (tag_drawn == \"END\") | (len(sentence_words)==30):\n",
    "            break\n",
    "        \n",
    "        # Get potential words to be chosen out of a selected tag\n",
    "        next_word = []\n",
    "        next_word_probabilities = []\n",
    "        next_word_dict = {}\n",
    "        for i in emission_probability: \n",
    "            if i.split(\"/\")[1] == tag_drawn:\n",
    "                word = i.split(\"/\")[0]\n",
    "                probability = emission_probability[i]\n",
    "                next_word.append(word)\n",
    "                next_word_probabilities.append(probability)\n",
    "                next_word_dict[word] = probability\n",
    "        \n",
    "        # Random pick of a word\n",
    "#         print(\"Word\")\n",
    "#         print(next_word)\n",
    "#         print(next_word_probabilities)\n",
    "        word_drawn = random.choices(next_word, next_word_probabilities)[0]\n",
    "\n",
    "        sentence_tags.append(tag_drawn)\n",
    "        sentence_words.append(word_drawn)\n",
    "        sentence_transition_probability.append(next_tag_dict[tag_drawn])\n",
    "        sentence_emission_probability.append(next_word_dict[word_drawn])\n",
    "        current_tag = next_tag\n",
    "        \n",
    "    total_probability = 1\n",
    "    for i in range(len(sentence_words)):\n",
    "        total_probability = total_probability * sentence_transition_probability[i] * sentence_emission_probability[i]\n",
    "    \n",
    "    sentence = sentence_words[0]\n",
    "    for i in range(1, len(sentence_words)):\n",
    "        sentence = sentence + \" \" + sentence_words[i]\n",
    "    return(sentence_tags, sentence_words, sentence_transition_probability, sentence_emission_probability, total_probability, sentence)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating sentence: 1\n",
      "Generating sentence: 2\n",
      "Generating sentence: 3\n",
      "Generating sentence: 4\n",
      "Generating sentence: 5\n"
     ]
    }
   ],
   "source": [
    "sentence_generation_file = open('../output/generated_sentences.txt', 'w')\n",
    "for i in range(1,6):\n",
    "    print(\"Generating sentence: {}\".format(i))\n",
    "    sentence_generation_file.write(\"Sentence: {}\\n\".format(1))\n",
    "    sentence_tags, sentence_words, sentence_transition_probability, \\\n",
    "        sentence_emission_probability, total_probability, sentence = generate_sentence(\n",
    "            train_transition_probability, train_emission_probability\n",
    "        )\n",
    "    sentence_generation_file.write(\"Words: \\n {} \\n \".format(sentence_words))\n",
    "    sentence_generation_file.write(\"Tags: \\n {} \\n \".format(sentence_tags))\n",
    "    sentence_generation_file.write(\"Transition probability \\n {} \\n \".format(sentence_transition_probability))\n",
    "    sentence_generation_file.write(\"Emission probability \\n {} \\n \".format(sentence_emission_probability))\n",
    "    sentence_generation_file.write(\"Total probability: {} \\n \\n\".format(total_probability))\n",
    "    sentence_generation_file.write(\"Sentence: \\n {} \\n \\n\".format(sentence))\n",
    "sentence_generation_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.5  \n",
    "For each word in the test dataset, derive the most probable POS tag sequence using the Viterbi algorithm; pseudo-code can be found in the textbook http://web.stanford.edu/ jurafsky/slp3/ed3book.pdf under Figure 8.5. Viterbi algorithm should be implemented following the pseudocode provided for reference.\n",
    "\n",
    "Hint: Traversing through back-pointer data structure at the end of algorithm would provide information about the best possible previous tag. So when you are at the second last word of the sentence, calling back-pointer here would give the tag information for the first word in the sentence.\n",
    "\n",
    "Submit the output in a file exactly with the following format (where each line contains no more than one pair):\n",
    "< sentenceID = 1 >\n",
    "word/tag\n",
    "word/tag\n",
    "....\n",
    "word/tag\n",
    "< EOS >\n",
    "< sentenceID = 2 >\n",
    "word/tag\n",
    "word/tag\n",
    "....\n",
    "word/tag\n",
    "< EOS >"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_state_transition_matrix(states, transition_probability):\n",
    "    state_transition_df = pd.DataFrame(0, index=states, columns=states)\n",
    "    for key in transition_probability:\n",
    "        state1, state2 = key.split(\" \")\n",
    "        state_transition_df.loc[state1, state2] = transition_probability[key]\n",
    "    return state_transition_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_word_state_emission_matrix(words, states, emission_probability):\n",
    "    word_state_emission_df = pd.DataFrame(0, index=words, columns=states)\n",
    "    for key in emission_probability:\n",
    "        split_tokens = key.split('/')\n",
    "        word = split_tokens[0]\n",
    "        tag = split_tokens[-1]\n",
    "        word_state_emission_df.loc[word, tag] = emission_probability[key]\n",
    "    return word_state_emission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viterbi_algorithm(words, states, state_transition_df, word_state_emission_df):\n",
    "    score_df = pd.DataFrame(0, index=range(len(words)), columns=states)\n",
    "    trace_df = pd.DataFrame(\"\", index=range(len(words)), columns=states)\n",
    "    for i in range(len(words)):\n",
    "        # Check for word existence\n",
    "        word = words[i]\n",
    "#         print(\"########## Word: {}\".format(words[i]))\n",
    "        if word not in word_state_emission_df.index:\n",
    "            print(\"Converting word: {} to UNK\".format(word))\n",
    "            word = 'UNK'\n",
    "        \n",
    "        # For initial word\n",
    "        if i == 0:\n",
    "            temp_word_state_emission_list = word_state_emission_df.loc[word, states].values.tolist()\n",
    "            temp_state_transition_df_list = state_transition_df.loc[\"START\", states].values.tolist()\n",
    "            temp_score = [temp_word_state_emission_list[i]*temp_state_transition_df_list[i] for i in range(len(temp_word_state_emission_list))]\n",
    "            score_df.loc[i, states] = temp_score\n",
    "            trace_df.loc[i, states] = [\"START\"] * len(states)\n",
    "#             print(score_df.loc[i, :][score_df.loc[i, :] > 0])\n",
    "#             print(\">>>>>>>>>>>>\")\n",
    "#             print(trace_df.loc[i, :][score_df.loc[i, :] > 0])\n",
    "#             print(\">>>>>>>>>>>>\")\n",
    "        else:\n",
    "            previous_row_probability_list = score_df.loc[i-1, :].values.tolist()\n",
    "            for j in range(len(states)):\n",
    "                current_state = states[j]\n",
    "                temp_word_state_emission_list = [word_state_emission_df.loc[word, current_state]] * len(states)\n",
    "                temp_state_transition_df_list = state_transition_df.loc[states, current_state].values.tolist()\n",
    "                temp_score_list = [\n",
    "                    previous_row_probability_list[m] * temp_word_state_emission_list[m]*temp_state_transition_df_list[m] \n",
    "                    for m in range(len(temp_word_state_emission_list))\n",
    "                ]\n",
    "                f = lambda k: temp_score_list[k]\n",
    "                arg_max_temp_score = max(range(len(temp_score_list)), key=f)\n",
    "                score_df.loc[i, current_state] = temp_score_list[arg_max_temp_score]\n",
    "                trace_df.loc[i, current_state] = states[arg_max_temp_score]\n",
    "#             print(score_df.loc[i, :][score_df.loc[i, :] > 0])\n",
    "#             print(\">>>>>>>>>>>>\")\n",
    "#             print(trace_df.loc[i, :][score_df.loc[i, :] > 0])\n",
    "#             print(\">>>>>>>>>>>>\")\n",
    "            \n",
    "    # Trace back\n",
    "    result = []\n",
    "    # Get the highest score for the last word\n",
    "    chosen_state = score_df.loc[len(words)-1, :].argmax()\n",
    "    result.append((words[len(words)-1], chosen_state))\n",
    "    trace_back_column = trace_df.loc[len(words)-1, chosen_state]\n",
    "    for i in range(len(words)-2, -1, -1):\n",
    "        result.append((words[i], trace_back_column))\n",
    "        trace_back_column = trace_df.loc[i, trace_back_column]\n",
    "    return result[::-1]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_data(file_name):\n",
    "    file_pointer = open(file_name, \"r\")\n",
    "    sentences = []\n",
    "    sentence = []\n",
    "    for line in file_pointer:\n",
    "        \n",
    "        # Remove duplicate spaces\n",
    "        file_line_content = re.sub(' +', ' ', line)\n",
    "\n",
    "        # Strip of begin and end spaces\n",
    "        file_line_content = file_line_content.strip()\n",
    "\n",
    "        # If line is not empty\n",
    "        if file_line_content != \"\":\n",
    "            if \"sentence ID\" in file_line_content:\n",
    "                continue\n",
    "            elif  \"EOS\" in file_line_content:\n",
    "                sentences.append(sentence)\n",
    "                sentence = []\n",
    "            else:\n",
    "                sentence.append(file_line_content)\n",
    "\n",
    "    file_pointer.close()\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "state_transition_df = get_state_transition_matrix(list(train_transition_tag_unigrams.keys()), train_transition_probability)\n",
    "del state_transition_df['START']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_state_emission_df = get_word_state_emission_matrix(list(train_word_unigrams.keys()), list(train_transition_tag_unigrams.keys()), train_emission_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 0\n",
      "Converting word: kilowatt-hour to UNK\n",
      "Converting word: kilowatts to UNK\n",
      "Converting word: kilowatt to UNK\n",
      "Converting word: $8 to UNK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/karangm/PycharmProjects/pos_tagging/venv/lib/python3.6/site-packages/ipykernel_launcher.py:45: FutureWarning: \n",
      "The current behaviour of 'Series.argmax' is deprecated, use 'idxmax'\n",
      "instead.\n",
      "The behavior of 'argmax' will be corrected to return the positional\n",
      "maximum in the future. For now, use 'series.values.argmax' or\n",
      "'np.argmax(np.array(values))' to get the position of the maximum\n",
      "row.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing sentence 1\n",
      "Converting word: kilowatt-hour to UNK\n",
      "Processing sentence 2\n",
      "Processing sentence 3\n",
      "Converting word: out-of-pocket to UNK\n",
      "Processing sentence 4\n",
      "Converting word: electric-utility to UNK\n",
      "Processing sentence 5\n",
      "Processing sentence 6\n",
      "Processing sentence 7\n",
      "Converting word: utility-cost to UNK\n",
      "Processing sentence 8\n",
      "Processing sentence 9\n",
      "Converting word: subtype to UNK\n",
      "Converting word: distributes to UNK\n",
      "Processing sentence 10\n",
      "Processing sentence 11\n",
      "Converting word: whereof to UNK\n",
      "Converting word: hereunto to UNK\n",
      "Converting word: 11th to UNK\n",
      "Converting word: sixty-one to UNK\n",
      "Converting word: eighty-sixth to UNK\n",
      "Processing sentence 12\n",
      "Converting word: Resolution to UNK\n",
      "Converting word: 22nd to UNK\n",
      "Converting word: Maritime to UNK\n",
      "Processing sentence 13\n",
      "Converting word: intermissions to UNK\n",
      "Processing sentence 14\n",
      "Processing sentence 15\n",
      "Converting word: whereof to UNK\n",
      "Converting word: hereunto to UNK\n",
      "Converting word: sixty-one to UNK\n",
      "Converting word: eighty-sixth to UNK\n",
      "Processing sentence 16\n",
      "Processing sentence 17\n",
      "Converting word: frugality to UNK\n",
      "Processing sentence 18\n",
      "Processing sentence 19\n",
      "Converting word: Pilgrims to UNK\n",
      "Processing sentence 20\n",
      "Processing sentence 21\n",
      "Converting word: Crombie to UNK\n",
      "Converting word: Blatz's to UNK\n",
      "Processing sentence 22\n",
      "Converting word: Crombie to UNK\n",
      "Processing sentence 23\n",
      "Converting word: Blatz to UNK\n",
      "Converting word: Smithtown to UNK\n",
      "Processing sentence 24\n",
      "Processing sentence 25\n",
      "Processing sentence 26\n",
      "Converting word: pegboard to UNK\n",
      "Processing sentence 27\n",
      "Processing sentence 28\n",
      "Processing sentence 29\n",
      "Processing sentence 30\n",
      "Processing sentence 31\n",
      "Processing sentence 32\n",
      "Converting word: Mattie to UNK\n",
      "Converting word: Toonker to UNK\n",
      "Converting word: Burkette to UNK\n",
      "Converting word: yanking to UNK\n",
      "Processing sentence 33\n",
      "Processing sentence 34\n",
      "Processing sentence 35\n",
      "Processing sentence 36\n",
      "Processing sentence 37\n",
      "Converting word: tramp to UNK\n",
      "Processing sentence 38\n",
      "Converting word: spellbound to UNK\n",
      "Processing sentence 39\n",
      "Processing sentence 40\n",
      "Processing sentence 41\n",
      "Processing sentence 42\n",
      "Converting word: Juanita to UNK\n",
      "Converting word: Lattimer to UNK\n",
      "Processing sentence 43\n",
      "Converting word: Randolph to UNK\n",
      "Converting word: Joel to UNK\n",
      "Converting word: replanted to UNK\n",
      "Converting word: Annie to UNK\n",
      "Processing sentence 44\n",
      "Processing sentence 45\n",
      "Processing sentence 46\n",
      "Processing sentence 47\n",
      "Processing sentence 48\n",
      "Converting word: Juanita's to UNK\n",
      "Processing sentence 49\n",
      "Processing sentence 50\n",
      "Converting word: 4,585 to UNK\n",
      "Converting word: Fisk to UNK\n",
      "Processing sentence 51\n",
      "Processing sentence 52\n",
      "Processing sentence 53\n",
      "Converting word: benchmarks to UNK\n",
      "Converting word: profoundity to UNK\n",
      "Processing sentence 54\n",
      "Processing sentence 55\n",
      "Converting word: libertarian to UNK\n",
      "Processing sentence 56\n",
      "Converting word: inalienable to UNK\n",
      "Processing sentence 57\n",
      "Converting word: Avowed to UNK\n",
      "Converting word: freethinkers to UNK\n",
      "Processing sentence 58\n",
      "Converting word: traditionalistic to UNK\n",
      "Processing sentence 59\n",
      "Converting word: socially-oriented to UNK\n",
      "Processing sentence 60\n",
      "Processing sentence 61\n",
      "Processing sentence 62\n",
      "Processing sentence 63\n",
      "Processing sentence 64\n",
      "Converting word: codified to UNK\n",
      "Processing sentence 65\n",
      "Processing sentence 66\n",
      "Converting word: Hesperus to UNK\n",
      "Converting word: Lucifer to UNK\n",
      "Processing sentence 67\n",
      "Processing sentence 68\n",
      "Converting word: Hesperus to UNK\n",
      "Processing sentence 69\n",
      "Processing sentence 70\n",
      "Processing sentence 71\n",
      "Processing sentence 72\n",
      "Processing sentence 73\n",
      "Converting word: Warmly to UNK\n",
      "Processing sentence 74\n",
      "Processing sentence 75\n",
      "Converting word: SX-21 to UNK\n",
      "Processing sentence 76\n",
      "Converting word: plain-clothesmen to UNK\n",
      "Processing sentence 77\n",
      "Converting word: Thor's to UNK\n",
      "Converting word: Antony to UNK\n",
      "Converting word: zing to UNK\n",
      "Processing sentence 78\n",
      "Processing sentence 79\n",
      "Processing sentence 80\n",
      "Converting word: Ought to UNK\n",
      "Converting word: edifying to UNK\n",
      "Converting word: Trial to UNK\n",
      "Converting word: anti-Semites to UNK\n",
      "Converting word: skull-bashings to UNK\n",
      "Converting word: gassings to UNK\n",
      "Processing sentence 81\n",
      "Processing sentence 82\n",
      "Converting word: patriots to UNK\n",
      "Converting word: terrorizing to UNK\n",
      "Converting word: meanest to UNK\n",
      "Converting word: pulverizing to UNK\n",
      "Processing sentence 83\n",
      "Converting word: Trial to UNK\n",
      "Converting word: anti-Semitic to UNK\n",
      "Converting word: demoralization to UNK\n",
      "Processing sentence 84\n",
      "Processing sentence 85\n",
      "Converting word: Wansee to UNK\n",
      "Converting word: Heydrich to UNK\n",
      "Processing sentence 86\n",
      "Converting word: Trial to UNK\n",
      "Processing sentence 87\n",
      "Converting word: Trial to UNK\n",
      "Converting word: anti-Semitism to UNK\n",
      "Processing sentence 88\n",
      "Converting word: anti-Semitism to UNK\n",
      "Converting word: Jew-baiter to UNK\n",
      "Processing sentence 89\n",
      "Converting word: Heydrich to UNK\n",
      "Converting word: Goering to UNK\n",
      "Converting word: Solution to UNK\n",
      "Converting word: strangulation to UNK\n",
      "Converting word: emigration to UNK\n",
      "Processing sentence 90\n",
      "Converting word: casualties to UNK\n",
      "Processing sentence 91\n",
      "Converting word: DePugh to UNK\n",
      "Converting word: Lauchli to UNK\n",
      "Processing sentence 92\n",
      "Converting word: Minutemen to UNK\n",
      "Processing sentence 93\n",
      "Processing sentence 94\n",
      "Converting word: Vietnam to UNK\n",
      "Processing sentence 95\n",
      "Converting word: Albanians to UNK\n",
      "Processing sentence 96\n",
      "Converting word: Malinovsky to UNK\n",
      "Converting word: exalting to UNK\n",
      "Processing sentence 97\n",
      "Processing sentence 98\n",
      "Processing sentence 99\n",
      "Converting word: liberating to UNK\n",
      "Processing sentence 100\n",
      "Converting word: squashed to UNK\n",
      "Converting word: suntan to UNK\n",
      "Converting word: semi-inflated to UNK\n",
      "Processing sentence 101\n",
      "Processing sentence 102\n",
      "Processing sentence 103\n",
      "Processing sentence 104\n",
      "Converting word: dirt-catcher to UNK\n",
      "Processing sentence 105\n",
      "Processing sentence 106\n",
      "Converting word: out-of-sight to UNK\n",
      "Converting word: out-of-mind to UNK\n",
      "Converting word: trek to UNK\n",
      "Processing sentence 107\n",
      "Converting word: Soignee to UNK\n",
      "Processing sentence 108\n",
      "Processing sentence 109\n",
      "Processing sentence 110\n",
      "Processing sentence 111\n",
      "Converting word: Jannequin's to UNK\n",
      "Converting word: tarantara to UNK\n",
      "Converting word: rum-tum-tum to UNK\n",
      "Converting word: boom-boom-boom to UNK\n",
      "Converting word: chansons to UNK\n",
      "Converting word: Jannequin to UNK\n",
      "Converting word: Lassus to UNK\n",
      "Processing sentence 112\n",
      "Converting word: Jean-Marie to UNK\n",
      "Converting word: LeClair to UNK\n",
      "Converting word: Bodin to UNK\n",
      "Converting word: Beismortier to UNK\n",
      "Converting word: Corrette to UNK\n",
      "Converting word: Mondonville to UNK\n",
      "Processing sentence 113\n",
      "Converting word: forego to UNK\n",
      "Processing sentence 114\n",
      "Converting word: out-of-the-way to UNK\n",
      "Processing sentence 115\n",
      "Converting word: dancelike to UNK\n",
      "Processing sentence 116\n",
      "Converting word: Elegance to UNK\n",
      "Processing sentence 117\n",
      "Processing sentence 118\n",
      "Converting word: Alvise to UNK\n",
      "Processing sentence 119\n",
      "Processing sentence 120\n",
      "Processing sentence 121\n",
      "Processing sentence 122\n",
      "Converting word: Disapproval to UNK\n",
      "Processing sentence 123\n",
      "Converting word: full-dress to UNK\n",
      "Processing sentence 124\n",
      "Processing sentence 125\n",
      "Processing sentence 126\n",
      "Processing sentence 127\n",
      "Processing sentence 128\n",
      "Processing sentence 129\n",
      "Processing sentence 130\n",
      "Converting word: Stacy to UNK\n",
      "Converting word: Forbes to UNK\n",
      "Processing sentence 131\n",
      "Processing sentence 132\n",
      "Processing sentence 133\n",
      "Processing sentence 134\n",
      "Converting word: Kimball to UNK\n",
      "Converting word: Stacy to UNK\n",
      "Processing sentence 135\n",
      "Processing sentence 136\n",
      "Processing sentence 137\n",
      "Converting word: Soak to UNK\n",
      "Processing sentence 138\n",
      "Converting word: gullet to UNK\n",
      "Processing sentence 139\n",
      "Converting word: Stacy to UNK\n",
      "Converting word: remarry to UNK\n",
      "Converting word: Forbes to UNK\n",
      "Processing sentence 140\n",
      "Processing sentence 141\n",
      "Converting word: Methodism to UNK\n",
      "Processing sentence 142\n",
      "Converting word: Incurably to UNK\n",
      "Converting word: devout to UNK\n",
      "Converting word: Greenleaf to UNK\n",
      "Converting word: Whittier to UNK\n",
      "Converting word: 1807-1892 to UNK\n",
      "Converting word: plenary to UNK\n",
      "Processing sentence 143\n",
      "Converting word: Oberlin to UNK\n",
      "Processing sentence 144\n",
      "Processing sentence 145\n",
      "Processing sentence 146\n",
      "Converting word: 1811-1884 to UNK\n",
      "Converting word: Lyman to UNK\n",
      "Converting word: Beecher to UNK\n",
      "Processing sentence 147\n",
      "Processing sentence 148\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting word: anti-slavery to UNK\n",
      "Converting word: Finney to UNK\n",
      "Converting word: revivals to UNK\n",
      "Processing sentence 149\n"
     ]
    }
   ],
   "source": [
    "test_sentences = get_test_data('../input/Test_File.txt')\n",
    "viterbi_output_file = open('../output/viterbi_output.txt', 'w')\n",
    "for i in range(len(test_sentences)):\n",
    "    print(\"Processing sentence {}\".format(i))\n",
    "    viterbi_output_file.write(\"< sentence ID = {} >\\n\".format(i+1))\n",
    "#     test_sentences[i] = ['Bella', 'wanted', 'to', 'board', 'the', 'bus', 'to', 'Chicago']\n",
    "#     test_sentences[i] = ['John', 'nailed', 'the', 'board', 'over', 'the', 'window']\n",
    "    result = viterbi_algorithm(test_sentences[i],\n",
    "                               list(train_tag_unigrams.keys()),\n",
    "                               state_transition_df,\n",
    "                               word_state_emission_df\n",
    "                            )\n",
    "    for word,tag in result:\n",
    "        viterbi_output_file.write(\"{}/{}\\n\".format(word,tag))\n",
    "    viterbi_output_file.write(\"< EOS >\\n\")\n",
    "viterbi_output_file.close()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pos_tagging",
   "language": "python",
   "name": "pos_tagging"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
